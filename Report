Part 1:

0.) 

The upper two loops can be easily parallelized. There won't be any two threads that will write to the same part of memory in matrix C when the 
upper two loops are parallelized so there are no critical sections that need atomicity. The third loop can also be parallelized but this will 
need atomicity since each scalar value computed (i,k) * (k,j) has to be added to the scalar value (i,j) where (i,j) has to be locked so at most
1 thread modifies it at any given time. 


1.) 

Code: mat_mult_openmp.c 
Compilation: gcc -o prog4_part1 mat_mult_openmp.c -fopenmp
-> The code  includes the in-serial and multithreaded matrix multiplication computaition. 
-> The multithreaded matrix multiplication parallelizes the upper two loops. 

2.) 
For N=1000, in-serial matrix multiplication returns 5.41 on average (5 runs) whereas using 1 thread with openmp
returns 5.53 on average (5 runs). It performs as we expect where openmp 1 thread has a very slight increase (almost 
negligible) in running time because of set-up costs.

3.) 
Plot (log-log scale) found in plot_part1_4.png

4.) 
Plot (log-log scale) found in plot_part1_4.png

-> The ideal scaling lines which are linear in log-log scale are plotted as dashed lines for every matrix size plot. 
-> We can observe that for matrix sizes of 1000 and 2000, the plots are nearly linear (computing the best-fit line would
   return a slope near 1.0) so the parallel speedup looks ideal (parallel scaling efficiency close 1). 
-> We can observe that for matrix size of 20, increasing the number of threads leads to an increase in running time since 
   the setup costs of threads and their synchronization dominates the running time cost as compared to how much parallelization 
   can be done.
-> For matrix size of 100, we can observe a near ideal speedup for thread counts 2 and 4 (parallel scaling efficiency near 1.0). 
   However, increasing the number of threads further starts ruining performance (increase in running time) since the computations
   have been parallelized as much as possible and the setup and synchronization costs of threads now dominate. 

5.)
-> Seed the random number generator using srand with a fixed value=3 srand(3).
-> For different number of threads (1,2, ... , 128), compute the multiplication of 2 matrices (size=1000) whose values
   were randomly generated and write the results to standard output.
-> Verify that the results across are the same. 


________________________________________________________________________________________________________________________

Part 2:

In part2 directory

________________________________________________________________________________________________________________________

Part 3:

1.) Code: part_3_code.c

2.)




